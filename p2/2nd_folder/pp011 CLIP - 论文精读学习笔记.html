<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<link rel="stylesheet" type="text/css" href="../../assets/markdownStyle/iconSetup.css">
	
	<!--右边底部的向上箭头，能够返回到文章最开始的地方2/2 动态效果-->
	<script type="text/javascript" src="../../assets/blogJS/wp-includes.js.jquery.jquery.js"></script>
	<script type="text/javascript" src="../../assets/blogJS/wp-content.themes.type-plus.js.main.js"></script>
	
	
	<!--https://www.dofactory.com/html/rel/icon-->
	<link rel="icon" href="../../images/ico/signature.png" sizes="32x32">
	<link rel="icon" href="../../images/ico/signature.png" sizes="192x192">
	<link rel="apple-touch-icon" href="../../images/ico/signature.png">
<link href='https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; --title-bar-height:20px; }
.mac-os-11 { --title-bar-height:28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
h1, h2, h3, h4, h5 { white-space: pre-wrap; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
thead, tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
svg { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; border-color: transparent !important; padding-top: 0px !important; padding-bottom: 0px !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  #write > p:nth-child(1) { margin-top: 0px; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
  figure { overflow-x: visible; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.reversefootnote { font-family: ui-monospace, sans-serif; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex:2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; overflow-wrap: anywhere; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }
mjx-container { break-inside: avoid; }


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

/*@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        content: "";
        display: block;
        height: 100px;
        margin-top: -96px;
        border-top: 1px solid #eee;
    }
}*/

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #ec962a;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table th:first-child,
table td:first-child {
    margin-top: 0;
}
table th:last-child,
table td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
	color: red;
    background-color: rgb(255, 255, 0)
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    pre {
        page-break-inside: avoid;
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

/*.html-for-mac {
    --item-hover-bg-color: #E6F0FE;
}*/

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
    opacity: 0.4;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}

.menu-item-container a.menu-style-btn {
    background-color: #f5f8fa;
    background-image: linear-gradient( 180deg , hsla(0, 0%, 100%, 0.8), hsla(0, 0%, 100%, 0)); 
}

u {
    text-decoration: red underline; 
	text-decoration-thickness: 15%;
  }
  
em {
	font-weight: bold;
    font-style: italic;
}
  


mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
  min-height: 1px;
  min-width: 1px;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line], svg[data-table] > g > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > g > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

mjx-container[jax="SVG2"] path[data-c], mjx-container[jax="SVG2"] use[data-c] {
  stroke-width: 3;
}

g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}

.MathJax g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}
mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
							stroke-width: 0;
						}
</style><title>pp011 CLIP - 论文精读学习笔记</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><h1 id='clip2021---论文精读学习笔记'><span>CLIP</span><sup class='md-footnote'><a href='#dfref-footnote-1' name='ref-footnote-1'>1</a></sup><span> - 论文精读学习笔记</span></h1><details style="background: none; padding: 20px; border: 2px solid #990000;border-radius: 25px; line-height:150%;"> <summary>Learning Transferable Visual Models From Natural Language Supervision</summary>标签：<kbd style="background:yellow; color:#990000">Multimodal Large Language Models</kbd><br/>论文链接：<a href="https://proceedings.mlr.press/v139/radford21a">Learning Transferable Visual Models From Natural Language Supervision</a><br/>官方项目/代码：<a href="https://github.com/openai/CLIP">CLIP</a> | <a href="https://huggingface.co/docs/transformers/model_doc/clip">Huggingface CLIP</a> | <a href="https://gitcode.com/gh_mirrors/cl/CLIP/overview">Gitcode CLIP</a><br/><span style="color:red">发表时间：</span><span style="color:blue; font-family:Comic Sans MS">2021</span></details><div style="text-align:center; font-size:1em" >
    <a href="https://mustbook.github.io/" style="color:#990000; font-weight:bold" >Cook</a><br/>
    <span style="color:#990000; font-family:Comic Sans MS; font-size:13px">Published: 2024.09.13</span><span style="color:blue"> | </span><span style="color:#990000; font-family:Comic Sans MS; font-size:13px">Last Updated: 2024.09.24</span>
</div><blockquote><p><i style="color:#990000; font-family:"><span>You are what you eat.</span><br/><span> And I&#39;m cooking what I eat!  </span></i><span> </span><strong><span>:)</span></strong><span> </span></p><p><span style="color:blue; font-family:Comic Sans MS"><a href='https://mustbook.github.io/p2/2nd_paper.html'><span>More food...</span></a></span>🍜<span> </span></p></blockquote><p style="text-align:center; font-size:20px; font-weight:bold;"> 目录 </p> <div class='md-toc' mdtype='toc'><p class="md-toc-content" role="list"><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n0"><a class="md-toc-inner" href="#clip2021---论文精读学习笔记">CLIP - 论文精读学习笔记</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n12"><a class="md-toc-inner" href="#全文概述">全文概述</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n52"><a class="md-toc-inner" href="#摘要">摘要</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n55"><a class="md-toc-inner" href="#背景">背景</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n71"><a class="md-toc-inner" href="#方法">方法</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n79"><a class="md-toc-inner" href="#clip是如何进行预训练的">CLIP是如何进行预训练的？</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n81"><a class="md-toc-inner" href="#clip是如何做zero-shot的推理的">CLIP是如何做zero-shot的推理的？</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n84"><a class="md-toc-inner" href="#natural-language-supervision">Natural Language Supervision</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n89"><a class="md-toc-inner" href="#creating-a-sufficiently-large-dataset---数据集">Creating a Sufficiently Large Dataset - 数据集</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n98"><a class="md-toc-inner" href="#为什么clip要采用对比学习的方法">为什么CLIP要采用对比学习的方法</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n107"><a class="md-toc-inner" href="#训练">训练</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n137"><a class="md-toc-inner" href="#整体架构">整体架构</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n138"><a class="md-toc-inner" href="#choosing-and-scaling-a-model">Choosing and Scaling a Model</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n140"><a class="md-toc-inner" href="#训练-2">训练</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n148"><a class="md-toc-inner" href="#主干模型">主干模型</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n158"><a class="md-toc-inner" href="#推理过程">推理过程</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n176"><a class="md-toc-inner" href="#代码">代码</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n187"><a class="md-toc-inner" href="#与之前zero-shot模型的对比">与之前Zero-shot模型的对比</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n195"><a class="md-toc-inner" href="#实验">实验</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n196"><a class="md-toc-inner" href="#zero-shot-transfer">Zero-Shot Transfer</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n197"><a class="md-toc-inner" href="#动机">动机</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n200"><a class="md-toc-inner" href="#using-clip-for-zero-shot-transfer">Using CLIP for zero-shot transfer</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n203"><a class="md-toc-inner" href="#initial-comparison-to-visual-n-grams">Initial comparison to visual N-Grams</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n206"><a class="md-toc-inner" href="#prompt-engineering-and-ensembling">Prompt engineering and ensembling</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n219"><a class="md-toc-inner" href="#analysis-of-zero-shot-clip-preformance">Analysis of zero-shot clip preformance</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n221"><a class="md-toc-inner" href="#大范围数据集结果">大范围数据集结果</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n231"><a class="md-toc-inner" href="#模型的泛化性">模型的泛化性</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n240"><a class="md-toc-inner" href="#不足">不足</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n262"><a class="md-toc-inner" href="#总结">总结</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n266"><a class="md-toc-inner" href="#补充">补充</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n276"><a class="md-toc-inner" href="#参考博文">参考博文</a></span></p></div><p><span style="color:blue; font-family:仿宋; font-weight:bold"><span>提前说明</span></span><span>：本系列博文主要是对</span><a href='#参考博文'><span>参考博文</span></a><span>的解读与重述（</span><em><span>对重点信息进行标记、或者整段摘录加深自己的记忆和理解、融合多个博文的精髓、统合不同的代表性的案例</span></em><span>），仅做学习记录笔记使用。与君共享，希望一同进步。</span></p><p>&nbsp;</p><h3 id='全文概述'><span>全文概述</span></h3><p><span>论文名：从自然语言监督中学习可转移的视觉模型</span></p><blockquote><p><span>通过自然语言处理来的一些监督信号，可以去训练一个迁移效果很好的视觉模型。</span></p></blockquote><p><span>想要解决的问题：之前的计算机视觉模型的数据集都是针对某一类特定任务，迁移效果较差，同时，一些训练时表现好的模型可能在测试中表现不佳。</span></p><p><kbd style="border:1px double black; font-size:20px; color: #990000; font-family: comic sans ms, 微软雅黑; font-weight:bold; border-bottom: 2px solid black; border-top: 2px solid black;"><span>主要贡献</span></kbd><span> </span></p><p><span>如何利用自然语言作为监督信号</span></p><ul><li><p><span>不预先定义标签类别，直接利用从互联网爬取的400 million 个image-text pair进行图文匹配任务的训练，并将其成功迁移应用于30个现存的计算机视觉——OCR、动作识别、细粒度分类。举例来说，无需利用ImageNet的数据进行训练，就可以达到ResNet-50在该数据集上有监督训练的效果。</span></p></li><li><p><span>利用language作为监督信号来学习视觉特征。</span></p></li></ul><p>&nbsp;</p><p><kbd style="border:1px dotted #990000; font-size:20px; color: red; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>CLIP算法的核心</span></kbd><span> 利用自然语言包含的监督信号来训练视觉模型。</span><span style="color:blue; font-family:仿宋; font-weight:bold"><span>CLIP 是涉及文字和图片的多模态领域的工作，从文本中得到监督信号，引导视觉分类的任务</span></span><span>。</span></p><p><span>论文方法的核心是在自然语言的监督中学习。</span></p><p><span style="color:blue; font-family:仿宋; font-weight:bold"><span>从自然语言中学习有几个潜在的好处</span></span><span>：</span></p><ul><li><p><span>和用于图像分类的标准标记相比，扩展自然语言监督要更容易，因为不要求标注内容需要采用机器学习的兼容模式；</span></p></li><li><p><span>自然语言可以在互联网上大量的文本中进行学习；</span></p></li><li><p><span>自然语言学习不仅是学习一种表现，同时也将其和语言表示联系起来，灵活的实现“zero-shot”转移。</span></p></li></ul><p><span style="color:blue; font-family:仿宋; font-weight:bold"><span>CLIP主要完成的任务</span></span><span>：</span></p><p><span>给定一幅图像，在32768个随机抽取的文本片段中，找到能匹配的那个文本。</span></p><p><span style="color:blue; font-family:仿宋; font-weight:bold"><span>CLIP为完成任务的做法</span></span><span>：</span></p><p><span>为了完成这个任务，CLIP这个模型需要学习识别图像中各种视觉概念，并将视觉概念将图片关联，也因此，CLIP可以用于几乎任意视觉人类任务。例如，一个数据集的任务为区分猫和狗，则CLIP模型预测图像更匹配文字描述“一张狗的照片”还是“一张猫的照片”。</span></p><p>&nbsp;</p><p><kbd style="border:1px double black; font-size:20px; color: #990000; font-family: comic sans ms, 微软雅黑; font-weight:bold; border-bottom: 2px solid black; border-top: 2px solid black;"><span>总结</span></kbd><span> 是一个 zero-shot 的视觉分类模型，预训练的模型在没有微调的情况下在下游任务上取得了很好的迁移效果。是在跨模态训练无监督中的开创性工作。</span></p><blockquote><p><span>zero-shot：是指零样本学习，在别的数据集上学习好了，直接迁移做分类。</span></p><p>👁‍🗨<span>pp005 GPT2 - 论文精读学习笔记.md → 零样本学习</span></p></blockquote><p><strong style="color:#6f0670; font-size:18px"><span>预训练部分</span></strong></p><p><span>预训练网络的输入是文字与图片的配对，每一张图片都配有一小句解释性的文字。</span></p><ul><li><p><span>将文字和图片分别通过一个编码器，得到向量表示。</span></p></li><li><p><span>这里的文本编码器就是 Transformer；而图片编码器既可以是 Resnet，也可以是 Vision transformer。</span></p></li></ul><p><span>预训练采用对比学习，匹配的图片-文本对为正样本，不匹配的为负样本。收集到的文本-图像对，分别经过Text-Encoder和Image-Encoder，然后通过点积计算一个batch中文本和图像两两之间的相似度，得到一个batch size x batch size的相似度矩阵，对角线上的相似度值就是正样本的相似度值，因此在训练过程中优化目标就是让正样本的相似度值尽可能大。</span></p><h4 id='摘要'><span>摘要</span></h4><p><span>现在的compute vision systems是训练模型来预测一组固定的、提前定义好的目标类别（比如ImageNet就是1000个类，COCO就是80个类），</span><span style="color:red"><span>这种监督方式比较受限</span></span><span>，</span><span style="border-bottom: 2px dashed FireBrick;"><span>因为需要额外的标记数据，限制了泛化性，如果有新的类别就要再去收集数据</span></span><span>。</span><strong style="color:#6f0670; font-size:18px"><span>直接从图像有关的原始文本中学习是一种很有前途的替代方案，它利用了更广泛的监督来源</span></strong><span>。</span><span style="color:blue; font-family:仿宋; font-weight:bold"><span>给一些文本描述和一些图像，预测哪个标题和哪个图像搭配，这个预训练任务是一种有效且可扩展的方式，在从互联网收集的4亿（图像、文本）对的数据集上从头开始学习，能学到SOTA的image representations</span></span><span>。预训练之后，natural language is used to reference learned visual concepts(or describe new ones) enabling zero-shot transfer of the model to 下游任务（自然语言引导模型去做物体的分类，分类不局限于已经学到的视觉概念，还能扩展到新的类别，从而现在学到的这个模型是能够直接在下游任务上做zero-shot的推理）。对超过30多个数据集进行测试来评估性能，涵盖</span><code>OCR</code><span>，</span><code>视频中的动作识别</code><span>、</span><code>geo-localization</code><span>和许多</span><code>细粒度的分类</code><span>，能轻松地转移到大多数任务，并且能达到和有监督的baseline方法差不多的性能，而无需任何数据集特定的训练。在ImageNet上，不使用那1.28 million训练样本，就能得到和有监督的ResNet-50差不多的结果。</span></p><p>&nbsp;</p><h3 id='背景'><span>背景</span></h3><p><span>之前的一些弱监督方法效果不好的原因主要在scale，即数据集的规模和模型的规模。</span></p><p><span>论文的作者团队收集了一个超级大的图像文本配对的数据集，有400 million个图片文本的配对， 模型最大用了ViT-large，提出了</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>CLIP</span></kbd><span>（Contrastive Language-Image Pre-training），是一种从自然语言监督中学习的有效方法，在模型上一共尝试了8个模型，从resnet到ViT，最小模型和最大模型之间的计算量相差约100倍，</span><span style="color:red"><span>迁移学习的效果基本和模型大小成正相关</span></span><span>。尝试了30个数据集，都能和之前的有监督的模型效果差不多甚至更好。</span></p><p>&nbsp;</p><p><span>OpenAI提出了DALL·E模型，该模型可以从包含大量概念的文本描述中生成相关图像。其名称DALL·E致敬了艺术家Salvador Dalí和皮克斯动画角色WALL·E。</span></p><p><span>DALL·E采用了基于Transformer的预训练结构，共有</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="3.394ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 1500 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-699-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-699-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-699-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-699-TEX-N-31"></use><use data-c="32" xlink:href="#MJX-699-TEX-N-32" transform="translate(500,0)"></use><use data-c="30" xlink:href="#MJX-699-TEX-N-30" transform="translate(1000,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>120</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">120</script><span>亿参数。</span><span style="color:blue; font-family:楷体; font-weight:bold;"><span>该模型同时接收图像和其文本描述作为输入</span></span><span>，</span><span style="border-top: 2px solid red; border-bottom: 2px solid red; border-left: 2px solid red; border-right: 2px solid red; font-weight:bold"><span>输入</span></span><span>使用</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="4.525ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2000 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-700-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-700-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-700-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path id="MJX-700-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-700-TEX-N-31"></use><use data-c="32" xlink:href="#MJX-700-TEX-N-32" transform="translate(500,0)"></use><use data-c="38" xlink:href="#MJX-700-TEX-N-38" transform="translate(1000,0)"></use><use data-c="30" xlink:href="#MJX-700-TEX-N-30" transform="translate(1500,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1280</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">1280</script><span>个token，包括</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="3.394ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 1500 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-701-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-701-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-701-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-701-TEX-N-32"></use><use data-c="35" xlink:href="#MJX-701-TEX-N-35" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-701-TEX-N-36" transform="translate(1000,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>256</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">256</script><span>个词汇量是</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="5.656ex" height="1.581ex" role="img" focusable="false" viewBox="0 -677 2500 699" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-702-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-702-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-702-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-702-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path id="MJX-702-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-702-TEX-N-31"></use><use data-c="36" xlink:href="#MJX-702-TEX-N-36" transform="translate(500,0)"></use><use data-c="33" xlink:href="#MJX-702-TEX-N-33" transform="translate(1000,0)"></use><use data-c="38" xlink:href="#MJX-702-TEX-N-38" transform="translate(1500,0)"></use><use data-c="34" xlink:href="#MJX-702-TEX-N-34" transform="translate(2000,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16384</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">16384</script><span>的文本token和</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="4.525ex" height="1.581ex" role="img" focusable="false" viewBox="0 -677 2000 699" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-703-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-703-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-703-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-703-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-703-TEX-N-31"></use><use data-c="30" xlink:href="#MJX-703-TEX-N-30" transform="translate(500,0)"></use><use data-c="32" xlink:href="#MJX-703-TEX-N-32" transform="translate(1000,0)"></use><use data-c="34" xlink:href="#MJX-703-TEX-N-34" transform="translate(1500,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1024</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">1024</script><span>个词汇量是</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="4.525ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2000 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-704-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path id="MJX-704-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-704-TEX-N-39" d="M352 287Q304 211 232 211Q154 211 104 270T44 396Q42 412 42 436V444Q42 537 111 606Q171 666 243 666Q245 666 249 666T257 665H261Q273 665 286 663T323 651T370 619T413 560Q456 472 456 334Q456 194 396 97Q361 41 312 10T208 -22Q147 -22 108 7T68 93T121 149Q143 149 158 135T173 96Q173 78 164 65T148 49T135 44L131 43Q131 41 138 37T164 27T206 22H212Q272 22 313 86Q352 142 352 280V287ZM244 248Q292 248 321 297T351 430Q351 508 343 542Q341 552 337 562T323 588T293 615T246 625Q208 625 181 598Q160 576 154 546T147 441Q147 358 152 329T172 282Q197 248 244 248Z"></path><path id="MJX-704-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="38" xlink:href="#MJX-704-TEX-N-38"></use><use data-c="31" xlink:href="#MJX-704-TEX-N-31" transform="translate(500,0)"></use><use data-c="39" xlink:href="#MJX-704-TEX-N-39" transform="translate(1000,0)"></use><use data-c="32" xlink:href="#MJX-704-TEX-N-32" transform="translate(1500,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>8192</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">8192</script><span>的图像token。</span></p><ul><li><p><span>对于文本token，使用标准的随机mask，通过GPT-3构造；</span></p></li><li><p><span>对于图像token，使用稀疏注意力(只计算某行、某列或局部)，训练时图像尺寸被调整为</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="9.553ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 4222.4 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-705-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-705-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-705-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-705-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-705-TEX-N-32"></use><use data-c="35" xlink:href="#MJX-705-TEX-N-35" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-705-TEX-N-36" transform="translate(1000,0)"></use></g><g data-mml-node="mo" transform="translate(1722.2,0)"><use data-c="D7" xlink:href="#MJX-705-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(2722.4,0)"><use data-c="32" xlink:href="#MJX-705-TEX-N-32"></use><use data-c="35" xlink:href="#MJX-705-TEX-N-35" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-705-TEX-N-36" transform="translate(1000,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>256</mn><mo>×</mo><mn>256</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">256 \times 256</script><span>，并参考VQ-VAE模型将其压缩为</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="7.291ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 3222.4 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-706-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-706-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-706-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="33" xlink:href="#MJX-706-TEX-N-33"></use><use data-c="32" xlink:href="#MJX-706-TEX-N-32" transform="translate(500,0)"></use></g><g data-mml-node="mo" transform="translate(1222.2,0)"><use data-c="D7" xlink:href="#MJX-706-TEX-N-D7"></use></g><g data-mml-node="mn" transform="translate(2222.4,0)"><use data-c="33" xlink:href="#MJX-706-TEX-N-33"></use><use data-c="32" xlink:href="#MJX-706-TEX-N-32" transform="translate(500,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>32</mn><mo>×</mo><mn>32</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">32 \times 32</script><span>的特征，使用字典对每个特征进行编码。整</span></p></li></ul><p><span>体模型采用</span><span style="color:red"><span>极大似然算法</span></span><span>进行</span><mark style="background:#f8e272;"><span>自回归训练</span></mark><span>。</span></p><p><span>该模型能够从不同的文本描述中生成对应的图像：</span></p><p><img src=".\pp011_files\image-20240924112548588.png" referrerpolicy="no-referrer" alt="image-20240924112548588"></p><p><span>上述结果是在生成的</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="3.394ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 1500 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-707-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-707-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-707-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="35" xlink:href="#MJX-707-TEX-N-35"></use><use data-c="31" xlink:href="#MJX-707-TEX-N-31" transform="translate(500,0)"></use><use data-c="32" xlink:href="#MJX-707-TEX-N-32" transform="translate(1000,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>512</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">512</script><span>个样本中选择的前</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 1000 688" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.05ex;"><defs><path id="MJX-708-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-708-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="33" xlink:href="#MJX-708-TEX-N-33"></use><use data-c="32" xlink:href="#MJX-708-TEX-N-32" transform="translate(500,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>32</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">32</script><span>个质量最好的样本，选择过程使用了CLIP模型。值得一提的是，目前的模型对纹理、颜色等性质生成的图像质量好，但对于数量、逻辑等性质生成的图像质量较差。</span></p><p>&nbsp;</p><h3 id='方法'><span>方法</span></h3><p><span>Contrastive Language-Image Pre-training (CLIP)方法用于在图像和文本数据集中进行匹配。</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red; background: yellow"><span>具体地，训练一个文本编码器和图像编码器，分别得到文本和图像的编码，并计算两者的匹配程度</span></span><span>。</span></p><ul><li><p><span>给定</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-721-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-721-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N</script><span>个图像-文本对，首先计算任意一个图像和文本之间的余弦相似度矩阵，尺寸为</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="6.784ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 2998.4 683" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-710-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-710-TEX-N-D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-710-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(1110.2,0)"><use data-c="D7" xlink:href="#MJX-710-TEX-N-D7"></use></g><g data-mml-node="mi" transform="translate(2110.4,0)"><use data-c="1D441" xlink:href="#MJX-710-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mo>×</mo><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N \times N</script><span>；</span></p></li><li><p><span>通过交叉熵损失使得匹配的</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-721-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-721-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N</script><span>个图像-文本对的相似度最大，其余</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="9.675ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4276.4 1000" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.566ex;"><defs><path id="MJX-712-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-712-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-712-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-712-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-712-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-712-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(888,0)"><use data-c="28" xlink:href="#MJX-712-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1277,0)"><use data-c="1D441" xlink:href="#MJX-712-TEX-I-1D441"></use></g><g data-mml-node="mo" transform="translate(2387.2,0)"><use data-c="2212" xlink:href="#MJX-712-TEX-N-2212"></use></g><g data-mml-node="mn" transform="translate(3387.4,0)"><use data-c="31" xlink:href="#MJX-712-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(3887.4,0)"><use data-c="29" xlink:href="#MJX-712-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container><script type="math/tex">N(N-1)</script><span>个相似度最小。</span></p></li></ul><p><img src=".\pp011_files\image-20240923234850136.png" referrerpolicy="no-referrer" alt="image-20240923234850136"></p><h4 id='clip是如何进行预训练的'><span>CLIP是如何进行预训练的？</span></h4><p><span>模型的</span><span style="border-top: 2px solid red; border-bottom: 2px solid red; border-left: 2px solid red; border-right: 2px solid red; font-weight:bold"><span>输入</span></span><span>是图片和文字的配对，图片输入到图片的encoder得到一些特征，文本输入到文本的encoder得到一些特征，每个traning batch里有n个图片-文本对，就能得到 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.025ex;"><defs><path id="MJX-714-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-714-TEX-I-1D45B"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">n</script><span> 个图片的特征和n个文本的特征，然后在这些特征上做对比学习，对比学习非常灵活，就需要正样本和负样本的定义，其它都是正常套路（不懂对比学习），配对的图片-文本对就是正样本，描述的是同一个东西，特征矩阵里对角线上的都是正样本，矩阵中非对角线上的元素都是负样本，</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.025ex;"><defs><path id="MJX-714-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-714-TEX-I-1D45B"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">n</script><span> 个正样本， </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="6.468ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 2859 915.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.186ex;"><defs><path id="MJX-715-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-715-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-715-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-715-TEX-I-1D45B"></use></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-715-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(1258.8,0)"><use data-c="2212" xlink:href="#MJX-715-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(2259,0)"><use data-c="1D45B" xlink:href="#MJX-715-TEX-I-1D45B"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>n</mi><mn>2</mn></msup><mo>−</mo><mi>n</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">n^2 - n</script><span> 个负样本，有了正负样本，模型就可以通过对比学习的方式去训练了，不需要任何手工标注。这种无监督的训练方式，是需要大量的训练数据的。</span></p><h4 id='clip是如何做zero-shot的推理的'><span>CLIP是如何做zero-shot的推理的？</span></h4><p><span>预训练之后只能得到文本和图片的特征，是没有分类头的，作者提出一种利用自然语言的方法，prompt template。比如对于ImageNet的类别，首先把它变成&quot;A photo of a {object}&quot; 这样一个句子，ImageNet有1000个类，就生成1000个句子，然后这1000个句子通过之前预训练好的文本的encoder能得到1000个文本特征。直接用类别单词去抽取文本特征也可以，但是模型预训练的时候和图片配对的都是句子，推理的时候用单词效果会下降。把需要分类的图片送入图片的encoder得到特征，拿图片的特征和1000个文本特征算余弦相似性，选最相似的那个文本特征对应的句子，从而完成了分类任务。不局限于这1000个类别，任何类别都可以。彻底摆脱了categorical label的限制，训练和推理的时候都不需要提前定义好的标签列表了。</span></p><p><span>在进行图片分类任务的推理时，由于没有分类头，作者利用自然语言进行分类，也就是prompt template，模型</span><kbd style="background:#f8e272"><span>1</span></kbd><span> 首先需要将类别标签转换成和预训练时候一样的句子，因此这里用到了prompt操作，获得类别相应的句子。</span><kbd style="background:#a8e195"><span>2</span></kbd><span> 最后计算输入图片和每个类别对应句子的相似度，相似度最高的句子对应的类别的就是预测的类别。</span></p><h4 id='natural-language-supervision'><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>Natural Language Supervision</span></kbd></h4><p><span style="color:red"><span>方法的核心</span></span><span>是the idea of learning perception from supervision contained in natural language.</span></p><p><span>相比其它的训练方法，从自然语言的监督信号来学习，有几个好处。</span></p><p><kbd style="background:#f8e272"><span>1</span></kbd><span> 首先就是it&#39;s much easier to scale，不需要再去标注数据，比如用</span><span style="border-bottom: 2px dashed FireBrick;"><span>传统方法做分类，需要先确定类别，然后去下载图片再清洗，再标注</span></span><span>，现在只需要去下载图片和文本的配对，数据集很容易就做大了，现在的监督对象是文本，而不是N选1的标签了。</span></p><p><kbd style="background:#a8e195"><span>2</span></kbd><span> 其次，it doesn&#39;t &quot;just&quot; learn a representation but also connects that representation to language which enables flexible zero-shot transfer. </span><span style="color:red"><span>训练的时候把图片和文本绑在了一起，学到的特征不再单是视觉特征了，而是多模态的特征，和语言连在一起以后，就很容易做zero-shot的迁移学习了</span></span><span>。</span><kbd style="border:1px double black; font-size:20px; color: #990000; font-family: comic sans ms, 微软雅黑; font-weight:bold; border-bottom: 2px solid black; border-top: 2px solid black;"><span>多模态</span></kbd></p><h5 id='creating-a-sufficiently-large-dataset---数据集'><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>Creating a Sufficiently Large Dataset</span></kbd><span> - 数据集</span></h5><p><span>a major motivation for natural language supervision是互联网上公开提供了这种形式的大量数据。</span><span style="color:red"><span>现有的数据集不能充分反应这种可能性</span></span><span>，因此仅考虑在这些数据集上的结果会低估这一研究方向的潜力。作者团队构建了400 million的图像文本对，这个数据集称作WIT（WebImage Text）。</span></p><p><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>Selection an Efficient Pre-Training Method</span></kbd></p><p><span>作者团队最初尝试的方法，类似VirTex，图像用CNN，文本用transformer，从头开始训练，预测图片的caption，</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red;"><span>这是一个预测型的任务</span></span><span>，然而在efficiently scaling this method的时候遇到了困难。作者实验发现一个具有63 million参数的transformer language model，已经使用了ResNet-50 image encoder两倍的算力，learns to recognize ImageNet classes three times slower than a much simpler baseline that predicts a bag-of-words encoding of the same text.</span></p><p><span>这个任务在试图预测每张图片所附文本的确切单词（给定一张图片，去预测对应文本，要逐字逐句去预测文本的），这是个困难的任务，因为与图像同时出现的描述、评论和相关文本种类繁多。</span><span style="color:blue; font-family:Comic Sans MS, 微软雅黑"><span>最近在图像对比学习方面的工作发现，contrastive objectives can learn better representations than their equivalent predictive objective. 其他一些研究发现，尽管图像的生成模型能够学习高质量的image representations，但它们的计算量比相同性能的对比模型要多一个数量级</span></span><span>。基于这些发现，本文探索训练了一个系统来解决可能更容易的任务，即只预测哪个文本作为一个整体和哪个图像配对，而不是预测该文本的确切单词，效率提高了4倍。</span></p><blockquote><p><span>科研好思路：转换问题，即，把困难的问题，转换为容易求解的问题。</span></p></blockquote><p><span>给定N个图片文本对，CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similarity of the image and text embeddings of th </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-721-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-721-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N</script><span> real pairs in the patch while minimizing the cosine similarity of the embeddings of the </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="7.894ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 3489.2 915.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.186ex;"><defs><path id="MJX-717-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-717-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-717-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-717-TEX-I-1D441"></use></g><g data-mml-node="mn" transform="translate(975.3,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-717-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(1601,0)"><use data-c="2212" xlink:href="#MJX-717-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(2601.2,0)"><use data-c="1D441" xlink:href="#MJX-717-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>N</mi><mn>2</mn></msup><mo>−</mo><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N^2 - N</script><span> incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores.</span></p><p>&nbsp;</p><h4 id='为什么clip要采用对比学习的方法'><span>为什么CLIP要采用对比学习的方法</span></h4><ul><li><p><span>OpenAI是一家从来不愁计算资源的公司，他们喜欢将一切都GPT化（就是做生成式模型）；</span></p></li><li><p><span>以往的工作表明（ResNeXt101-32x48d, Noisy Student EfficientNet-L2），训练资源往往需要很多，何况这些都只是在ImageNet上的结果，只是1000类的分类任务，</span><span style="color:red"><span>而CLIP要做的是开发世界的视觉识别任务，所以训练的效率对于自监督的模型至关重要</span></span><span>；而如果任务改为给定一张图片去预测一个文本（或者给定一个文本去预测一张图片），那么训练效率将会非常低下（因为一个图片可能对应很多种说法，一个文本也对应着很多种场景）；</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red;"><span>所以与其做默写古诗词，不如做选择题！</span></span><span>（只要判断哪一个文本与图片配对即可）；</span></p></li><li><p><span>通过从</span><mark style="background:#f8e272;"><span>预测任务</span></mark><span>改为</span><mark style="background:#a8e195;"><span>只预测某个单词</span></mark><span>到</span><mark style="background:#a5c4ff;"><span>只选出配对的答案</span></mark><span>，</span><span style="border-bottom: 2px dotted FireBrick;"><span>模型的训练效率一下提升了4倍</span></span><span>；</span></p><p><img src=".\pp011_files\image-20240917224641784.png" referrerpolicy="no-referrer" alt="image-20240917224641784"></p></li></ul><h4 id='训练'><span>训练</span></h4><p><span>CLIP在训练时使用了</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="1.131ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 500 677" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-718-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><use data-c="34" xlink:href="#MJX-718-TEX-N-34"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4</mn></math></mjx-assistive-mml></mjx-container><script type="math/tex">4</script><span>亿张从互联网收集的图像-文本对(WebImageText, WIT)，文本字典存储了在Wikipedia中出现超过100次的所有单词。</span></p><p><kbd style="border:1px dotted #990000; font-size:20px; color: red; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>结果</span></kbd><span> 实验结果表明，对于文本编码器，采用bag-of-words (BoW)模型比Transformer的效率能提高3倍；采用对比损失比直接预测图像对应的文本效率能提高4倍。</span></p><p><span>CLIP训练完成后可以实现zero-shot的推理，即不经过微调的迁移学习，该过程是通过prompt templete实现的。以ImageNet数据集的分类任务为例，对于一千个类别标签，分别生成一千个对应的文本(如A photo of a #Class)；通过CLIP匹配相似度最高的图像和文本，即可确定图像中出现目标的类别。</span></p><p>&nbsp;</p><p><span>CLIP核心实现的伪代码：</span></p><p><img src=".\pp011_files\image-20240917224659594.png" referrerpolicy="no-referrer" alt="image-20240917224659594"></p><blockquote><p><kbd style="background:yellow; color:red"><span>博文2</span></kbd><span> </span></p><ul><li><p><span>有两个输入，一个是图片，一个是文本。图片的维度是[n,h,w,c]，文本的维度是[n,l]，l是指序列长度，然后</span><mark><span>送入</span></mark><span>到各自的encoder提取特征，</span><strong><span>image encoder</span></strong><span>可以是</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>ResNet</span></kbd><span>也可以是</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>Vision Transformer</span></kbd><span>，text encoder可以是</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>CBOW</span></kbd><span>，也可以是</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>Text Transformer</span></kbd><span>，得到对应的特征之后，再经过一个投射层（即W_i和W_t)，投射层的意义是学习如何从单模态变成多模态，投射完之后再做l2 norm，就得到了最终的用来对比的特征 I_e 和 T_e ，现在有n个图像的特征，和n个文本的特征，接下来就是算consine similarity，算的相似度就是最后要分类的logits，最后logits和ground truth做交叉熵loss，正样本是对角线上的元素，logits的维度是[n,n]，ground truth label是np.arange(n)，（这里没有懂为什么np.arange(n)这样的ground truth），算两个loss，一个是image的，一个是text的，最后把两个loss加起来就平均。这个操作在对比学习中是很常见的，都是用的这种对称式的目标函数。</span></p></li><li><p><span>数据集很大很大，over-fitting不是主要问题，训练CLIP的细节也比较简单。从头开始训练，文本和图片的encoder都不需要使用预训练的weights，between the representation and the constastive embedding space也没有使用非线性的投射（projection），use only a linear projection to map from each encoder&#39;s representation to the multi-modal embedding space. 在之前对比学习的一些文章中提到过，</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>非线性投射层</span></kbd><span>比</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>线性投射层</span></kbd><span>能够带来将近10个点的性能提升，但是在CLIP中，作者发现线性还是非线性关系不大，他们怀疑非线性的投射层是用来适配纯图片的单模态学习的。也不需要做太多的数据增强，唯一用的是</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>随机裁剪</span></kbd><span>（a random square crop from resized images）。模型与数据集都实在是太大了，也不太好做调参的工作，在之前的对比学习中起到很重要作用的一个参数 </span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>temperature</span></kbd><span>（controls the range of the logits in the softmax, </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewBox="0 -431 517 444" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.029ex;"><defs><path id="MJX-719-TEX-I-1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D70F" xlink:href="#MJX-719-TEX-I-1D70F"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">\tau</script><span>），作者把它设置成了可学习的标量，直接在模型训练过程中优化，不需要调参。</span></p></li></ul></blockquote><ul><li><p><span>一个图片经过Image_encoder得到特征</span><span style="color:blue; font-family:Comic Sans MS"><span>If</span></span><span>，一个文本经过text_encoder得到特征</span><span style="color:blue; font-family:Comic Sans MS"><span>Tf</span></span><span>；</span></p></li><li><p><span>两个特征分别经过不同的FC层（目的是将单模态的特征转化为多模态，因为图片的特征可能本身就与文本的不一致，需要转换，但是这里没接激活函数，因为作者发现在多模态下接不接都一样）；</span></p></li><li><p><span>再做一次L2归一化；</span></p></li><li><p><span>计算余弦相似度，得到logits；</span></p></li><li><p><span>logits与GT计算交叉熵目标函数；</span></p></li><li><p><span>而这里的GT就是一个单位阵（因为目标是配对样本之间相似性最强为1，而其他为0）；</span></p></li><li><p><span>最后将图片的loss与文本的loss加起来求平均即可；</span></p></li></ul><p>&nbsp;</p><h4 id='整体架构'><span>整体架构</span></h4><h5 id='choosing-and-scaling-a-model'><span>Choosing and Scaling a Model</span></h5><p><kbd style="background:yellow; color:red"><span>博文2</span></kbd><span> （这一部分感觉不重要，也没太看懂，主要就将image encoder选了ResNet和ViT两种结构，text encoder只用了transformer）</span></p><h5 id='训练-2'><span>训练</span></h5><p><span>图片这边共训练了8个模型，5个ResNet和3个transformer：</span></p><ul><li><p><span>5个ResNet包括ResNet-50，ResNet-101；另外三个是根据efficientNet的方式对ResNet-50的宽度、深度、输入大小进行scale，分别对应原始ResNet50 4倍，16倍，64倍的计算量；</span></p></li><li><p><span>3个transformer包括ViT-B/32，ViT-B/16 和ViT-L/14。</span></p></li></ul><p><span>所有的模型都训练了32个epoch，用的adam优化器，超参数是基于grid searches，random search和manual tuning来调整的，为了让调参更快，超参搜索的时候是用的Res50，只训练了一个epoch。batch size 32768，非常大，也用到了混合精度训练（更快更省内存）。还做了很多工程上的优化来加速的。最大的那个ResNet（RN50x64）在592个V100的GPU上训练了18天，最大的ViT在256个V100 GPU上只花了12天。对预训练好的ViT-L/14，又在这个数据集上fine-tune了一个epoch，用的是更大尺寸（336*336的），这个模型称作ViT-L/14@336px，论文后面提到的CLIP除非特别说明，均指ViT-L/14@336px。</span></p><h5 id='主干模型'><span>主干模型</span></h5><p><span>在</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red;"><span>文本方面</span></span><span>就是Transformer；</span></p><p><span>在</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red;"><span>图像方面</span></span><span>选择了5种ResNets（ResNet-50，ResNet-101，3个EfficientNet的变体，ResNet-50x4，ResNet-50x16，ResNet-50x64）和三种VIT（分贝是VIT-B/32,VIT-B/16,VIT-L/14）。</span></p><p><img src=".\pp011_files\image-20240917225116494.png" alt="image-20240917225116494" style="zoom: 80%;" /></p><p><span>假设有 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-721-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-721-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N</script><span> 个图片和 </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: 0px;"><defs><path id="MJX-721-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D441" xlink:href="#MJX-721-TEX-I-1D441"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container><script type="math/tex">N</script><span> 个文本的编码。需要对它们进行对比学习（contrastive learning）。</span></p><ul><li><p><mark style="background:#f8e272;"><span>配对的句子</span></mark><span>和文本就是一对正样本（也就是对角线上的）；</span><mark style="background:#f8e272;"><span>不配对</span></mark><span>的就是负样本（对角线之外的）。</span></p></li><li><p><span>预训练网络的</span><strong style="color:#6f0670; font-size:18px"><span>目标</span></strong><span>，就是最大化正样本对的余弦相似度，并最小化负样本的余弦相似度。</span></p></li></ul><h5 id='推理过程'><span>推理过程</span></h5><p><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red; background: yellow"><span>CLIP 文章的核心 = Zero-shot Transfer</span></span><span>！！</span></p><p><span>作者研究迁移学习的动机：</span></p><blockquote><p><span>之前自监督or无监督的方法，主要研究 </span><span style="color:blue; font-family:Times New Roman, 仿宋; font-weight:bold"><span>feature 学习的能力</span></span><span>，model的目标是学习泛化性能好的特征，虽然学习到good-feature，但down-work中，还是需要有标签数据做微调。</span></p><p><span>作者想仅训练一个model，在down-work中不再微调。</span></p></blockquote><p><span>给定一张图片，如何利用预训练好的网络去做分类呢？</span></p><ul><li><p><span>给网络一堆分类标签，利用文本编码器得到向量表示。分别计算这些标签与图片的余弦相似度；最终相似度最高的标签即是预测的分类结果。</span></p></li></ul><p><span>相比于单纯地给定分类标签，给定一个句子的分类效果更好。比如一种句子模板 A photo of a ...，后面填入分类标签。这种句子模板叫做 prompt（提示）。</span></p><p><img src=".\pp011_files\image-20240917225518232.png" alt="image-20240917225518232" style="zoom:80%;" /></p><ul><li><p><span>将要做的分类以填空的形式填进一句话中，以ImageNet为例</span><span style="border-bottom: 2px dotted FireBrick;"><span>就是1000句话</span><span style="border-top: 2px solid red; border-bottom: 2px solid red; border-left: 2px solid red; border-right: 2px solid red; font-weight:bold"><span>输入</span></span><span>Text Encoder得到</span><span style="border-top: 2px solid black; border-bottom: 2px solid black; border-left: 2px solid black; border-right: 2px solid black; color:red; font-weight:bold"><span>输出</span></span></span><span>；</span></p></li><li><p><span>将要识别的图片经过Image Encoder得到图片输出，比较文本的输出与图片的输出，选择最相似的那句话就是图片的类别；</span></p></li></ul><p>&nbsp;</p><h4 id='代码'><span>代码</span></h4><p><span>当使用训练好的CLIP模型进行推理时，首先需要使用</span><code>clip.load()</code><span>加载模型，然后分别对图像和文本进行前处理。</span></p><ul><li><p><span>文字前处理是对数据集中的所有类别进行prompt engineering处理，将每个类别转换成句子，</span><code>clip.tokenize()</code><span>将句子长度padding到77个token长度。</span></p></li><li><p><span>下面的代码中，图像选取了CIFAR100数据集中的其中一张。</span></p></li><li><p><span>接下来，将图像和文字分别喂入图像编码器和文字编码器，提取图像特征和文字特征，分别将图像特征和文字特征正则化之后，计算图像特征和文字特征之间的相似度，并对相似度进行</span><code>softmax操作</code><span>。</span></p></li></ul><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang="python" style="break-inside: unset;"><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang="python"><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.26172px; left: 44px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 36px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>40</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation" style=""><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -36px; width: 36px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 27px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment"># 下面代码使用CLIP进行zero shot预测，从CIFAR-100数据集中挑选一张图片，预测这张图像最有可能与此数据集中100个标签中哪一个标签最相似。</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">2</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">import</span> <span class="cm-variable">os</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">3</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">import</span> <span class="cm-variable">clip</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">4</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">import</span> <span class="cm-variable">torch</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">5</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">from</span> <span class="cm-variable">torchvision</span>.<span class="cm-property">datasets</span> <span class="cm-keyword">import</span> <span class="cm-variable">CIFAR100</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">6</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> </span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">7</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment"># Load the model</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">8</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">device</span> <span class="cm-operator">=</span> <span class="cm-string">"cuda"</span> <span class="cm-keyword">if</span> <span class="cm-variable">torch</span>.<span class="cm-property">cuda</span>.<span class="cm-property">is_available</span>() <span class="cm-keyword">else</span> <span class="cm-string">"cpu"</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">9</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">model</span>, <span class="cm-variable">preprocess</span> <span class="cm-operator">=</span> <span class="cm-variable">clip</span>.<span class="cm-property">load</span>(<span class="cm-string">'ViT-B/32'</span>, <span class="cm-variable">device</span>) <span class="cm-comment"># 加载模型，返回的preprocess中包含一个torchvision transform依次执行Resize，CenterCrop和Normalization等操作。</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 27px;">10</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> </span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">11</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment"># Download the dataset</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">12</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">cifar100</span> <span class="cm-operator">=</span> <span class="cm-variable">CIFAR100</span>(<span class="cm-variable">root</span><span class="cm-operator">=</span><span class="cm-variable">os</span>.<span class="cm-property">path</span>.<span class="cm-property">expanduser</span>(<span class="cm-string">"~/.cache"</span>), <span class="cm-variable">download</span><span class="cm-operator">=</span><span class="cm-keyword">True</span>, <span class="cm-variable">train</span><span class="cm-operator">=</span><span class="cm-keyword">False</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">13</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> </span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">14</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment"># Prepare the inputs</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">15</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">image</span>, <span class="cm-variable">class_id</span> <span class="cm-operator">=</span> <span class="cm-variable">cifar100</span>[<span class="cm-number">3637</span>]</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">16</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">image_input</span> <span class="cm-operator">=</span> <span class="cm-variable">preprocess</span>(<span class="cm-variable">image</span>).<span class="cm-property">unsqueeze</span>(<span class="cm-number">0</span>).<span class="cm-property">to</span>(<span class="cm-variable">device</span>) <span class="cm-comment"># 图像前处理</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">17</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">text_inputs</span> <span class="cm-operator">=</span> <span class="cm-variable">torch</span>.<span class="cm-property">cat</span>([<span class="cm-variable">clip</span>.<span class="cm-property">tokenize</span>(<span class="cm-string">f"a photo of a </span>{<span class="cm-variable">c</span>}<span class="cm-string">"</span>) <span class="cm-keyword">for</span> <span class="cm-variable">c</span> <span class="cm-keyword">in</span> <span class="cm-variable">cifar100</span>.<span class="cm-property">classes</span>]).<span class="cm-property">to</span>(<span class="cm-variable">device</span>) <span class="cm-comment"># 文字前处理</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">18</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> </span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">19</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment"># Calculate features</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 27px;">20</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">with</span> <span class="cm-variable">torch</span>.<span class="cm-property">no_grad</span>():</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">21</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-variable">image_features</span> <span class="cm-operator">=</span> <span class="cm-variable">model</span>.<span class="cm-property">encode_image</span>(<span class="cm-variable">image_input</span>) <span class="cm-comment"># 图像编码器提取输入图像的特征</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">22</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-variable">text_features</span> <span class="cm-operator">=</span> <span class="cm-variable">model</span>.<span class="cm-property">encode_text</span>(<span class="cm-variable">text_inputs</span>) <span class="cm-comment"># 文字编码器提取输入文字的特征</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">23</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> </span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">24</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment cm-error"># Pick the top 5 most similar labels for the image</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">25</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">image_features</span> <span class="cm-operator">/=</span> <span class="cm-variable">image_features</span>.<span class="cm-property">norm</span>(<span class="cm-variable">dim</span><span class="cm-operator">=-</span><span class="cm-number">1</span>, <span class="cm-variable">keepdim</span><span class="cm-operator">=</span><span class="cm-keyword">True</span>) <span class="cm-comment"># 正则化</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">26</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">text_features</span> <span class="cm-operator">/=</span> <span class="cm-variable">text_features</span>.<span class="cm-property">norm</span>(<span class="cm-variable">dim</span><span class="cm-operator">=-</span><span class="cm-number">1</span>, <span class="cm-variable">keepdim</span><span class="cm-operator">=</span><span class="cm-keyword">True</span>) <span class="cm-comment"># 正则化</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">27</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">similarity</span> <span class="cm-operator">=</span> (<span class="cm-number">100.0</span> <span class="cm-operator">*</span> <span class="cm-variable">image_features</span> <span class="cm-operator">@</span> <span class="cm-variable">text_features</span>.<span class="cm-property">T</span>).<span class="cm-property">softmax</span>(<span class="cm-variable">dim</span><span class="cm-operator">=-</span><span class="cm-number">1</span>) <span class="cm-comment">#计算图像特征与文字特征之间的相似度</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">28</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">values</span>, <span class="cm-variable">indices</span> <span class="cm-operator">=</span> <span class="cm-variable">similarity</span>[<span class="cm-number">0</span>].<span class="cm-property">topk</span>(<span class="cm-number">5</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">29</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> </span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 27px;">30</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-comment"># Print the result</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">31</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-builtin">print</span>(<span class="cm-string">"\nTop predictions:\n"</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">32</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-keyword">for</span> <span class="cm-variable">value</span>, <span class="cm-variable">index</span> <span class="cm-keyword">in</span> <span class="cm-builtin">zip</span>(<span class="cm-variable">values</span>, <span class="cm-variable">indices</span>):</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">33</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-builtin">print</span>(<span class="cm-string">f"</span>{<span class="cm-variable">cifar100</span>.<span class="cm-property">classes</span>[<span class="cm-variable">index</span>]:<span class="cm-operator">&gt;</span><span class="cm-number">16</span><span class="cm-variable">s</span>}<span class="cm-string">: </span>{<span class="cm-number">100</span> <span class="cm-operator">*</span> <span class="cm-variable">value</span>.<span class="cm-property">item</span>():<span class="cm-number">.2</span><span class="cm-variable">f</span>}<span class="cm-string">%"</span>)</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">34</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">The</span> <span class="cm-variable">output</span> <span class="cm-variable">will</span> <span class="cm-variable">look</span> <span class="cm-variable">like</span> <span class="cm-variable">the</span> <span class="cm-variable">following</span> (<span class="cm-variable">the</span> <span class="cm-variable">exact</span> <span class="cm-variable">numbers</span> <span class="cm-variable">may</span> <span class="cm-variable">be</span> <span class="cm-variable">slightly</span> <span class="cm-variable">different</span> <span class="cm-variable">depending</span> <span class="cm-variable">on</span> <span class="cm-variable">the</span> <span class="cm-variable">compute</span> <span class="cm-variable">device</span>):</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">35</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"><span class="cm-variable">Top</span> <span class="cm-variable">predictions</span>:</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">36</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span class="cm-variable">snake</span>: <span class="cm-number">65.31</span><span class="cm-operator">%</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">37</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable">turtle</span>: <span class="cm-number">12.29</span><span class="cm-operator">%</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">38</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp;<span class="cm-variable">sweet_pepper</span>: <span class="cm-number">3.83</span><span class="cm-operator">%</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 27px;">39</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span class="cm-variable">lizard</span>: <span class="cm-number">1.88</span><span class="cm-operator">%</span></span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -36px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 27px;">40</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;"> &nbsp; &nbsp; &nbsp; <span class="cm-variable cm-error">crocodile</span>: <span class="cm-number">1.75</span><span class="cm-operator">%</span></span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 1037px;"></div><div class="CodeMirror-gutters" style="height: 1037px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 35px;"></div></div></div></div></pre><p>&nbsp;</p><h5 id='与之前zero-shot模型的对比'><span>与之前Zero-shot模型的对比</span></h5><p><img src=".\pp011_files\image-20240917230013426.png" referrerpolicy="no-referrer" alt="image-20240917230013426"></p><p><kbd style="background:yellow; color:red"><span>问题</span></kbd><span> Prompt 方法在什么时候用？</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang=""><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang=""><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.52344px; left: 36px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 28px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>1</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -28px; width: 28px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -28px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 19px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">Prompt是提示的意思，对model进行微调和直接做推理时有效；</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 23px;"></div><div class="CodeMirror-gutters" style="height: 23px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 27px;"></div></div></div></div></pre><p><kbd style="background:yellow; color:red"><span>问题</span></kbd><span> 为什么要用 prompt engineering and ensembling？</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang=""><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang=""><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.52344px; left: 36px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 28px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>3</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -28px; width: 28px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -28px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 19px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">由于一个word 具有多义性，图片和文字匹配容易出错，所以作者将word放在语境中，来提高匹配度；</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -28px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt" style="left: 0px; width: 19px;">2</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">Prompt不仅能做匹配；</span></pre></div><div style="position: relative;"><div class="CodeMirror-gutter-wrapper" style="left: -28px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 19px;">3</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">一旦加入这个prompt engineering and ensembling，准确度上升了1.3%；</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 69px;"></div><div class="CodeMirror-gutters" style="height: 69px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 27px;"></div></div></div></div></pre><p><span style="color:blue; font-family:Comic Sans MS, 微软雅黑"><span>最后在CLIP中，总共用了80个prompt template之多！</span></span></p><p>&nbsp;</p><h3 id='实验'><span>实验</span></h3><h4 id='zero-shot-transfer'><span>Zero-Shot Transfer</span></h4><h5 id='动机'><span>动机</span></h5><p><span>在计算机视觉中，</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>zero-shot学习</span></kbd><span>主要指 the study of generalizing to unseen object categories in image classification. 本文在更广的意义上使用这个术语，并研究对unseen datasets的泛化。</span></p><p><span>之前的那些自监督和无监督的方法，</span><span style="color:red"><span>主要研究的是特征学习的能力，</span><mark><span>目标</span></mark><span>就是学一种泛化性比较好的特征</span></span><span>，但即使学到了很好的特征，想应用到下游任务，还是需要有标签的数据做微调，所以有限制，比如下游任务数据不好收集，可能有distribution shift的问题。怎么做到只训练一个模型，后面不再需要微调了呢，这就是作者研究</span><kbd style="border:1px dashed #990000; font-size:15px; color: #00b456; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>zero-shot迁移</span></kbd><span>的研究动机。借助文本训练了一个又大又好的模型之后，就可以借助这个文本作为引导，很灵活的做zero-shot的迁移学习。</span></p><h5 id='using-clip-for-zero-shot-transfer'><span>Using CLIP for zero-shot transfer</span></h5><p><span>参考</span><span style="color:blue; font-weight:bold; font-style:italic; font-family:华文新魏; font-size:15px"><span>Figure 1</span></span><span> </span></p><p><kbd style="border:1px solid #990000; font-size:20px; color: blue; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>这个例子说的很好</span></kbd><span> 在clip预训练好之后，就有2个编码器，一个是</span><span style="color:red"><span>图像编码器</span></span><span>，一个是</span><span style="color:red"><span>文本编码器</span></span><span>，推理时给定一张图片，通过编码器就能得到一个图片的特征，文本那边的输入就是感兴趣的标签有哪些，比如plane，car，dog等，这些词会通过prompt engineering得到对应的句子，比如 &#39;A photo of a plane&#39;, &#39;A photo of a dog&#39;，有了这些句子以后，送入到文本编码器，就能得到对应的文本特征，这里假设是plane，car，dog这3个，然后拿这3个文本的特征去和那张图片的特征做余弦相似度，计算得到相似度以后再通过一个softmax得到概率分布，概率最大的那个句子就是在描述这张照片。</span></p><h5 id='initial-comparison-to-visual-n-grams'><span>Initial comparison to visual N-Grams</span></h5><p><span>visual N-Grams在ImageNet上只有11.5%的准确率，而CLIP已经达到了76.2%，和ResNet50精度差不多，完全没有用1.28 million的训练图片，直接zero-shot迁移就能得到76.2%。</span></p><p><span>当然作者也指出了这种对比不公平，CLIP的数据集是之前的10倍，而且视觉上的模型比之前要多100倍的计算，CLIP用的是transformer，2017年visual N-Grams发表的时候transformer还没出现。</span></p><h5 id='prompt-engineering-and-ensembling'><span>Prompt engineering and ensembling</span></h5><p><span>prompt是在做微调或者直接做推理时的一种方法，起到的是一个</span><span style="color:blue; font-family:仿宋; font-weight:bold"><span>提示的作用，也就是文本的引导作用</span></span><span>。</span></p><p><strong style="color:#6f0670; font-size:18px"><span>[为什么]</span></strong><span> 为什么要用Prompt engineering和Prompt ensembling呢？</span></p><ul><li><p><span>一个原因是polysemy，多义性，即一个单词可能同时有很多含义，如果在做文本和图片匹配的时候，每次只用一个单词，也就是标签对应的那个单词，去做文本的特征抽取，就有可能遇到问题，</span><span style="color:blue; font-family:仿宋; font-weight:bold"><span>因为缺乏上下文信息，模型无法区分是哪个词义</span></span><span>，</span><kbd style="border:1px dotted #990000; font-size:20px; color: red; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>案例</span></kbd><span> 比如在ImageNet里面同时包含两个类，construction cranes和cranes，在不同的语境下，这两个crane对应的意义是不一样的，在建筑工地环境下，construction cranes指的是起重机，作为动物，cranes指的是鹤，这就有歧义了。在Oxfort-IIIT Pet数据集中，有一类是boxer，根据上下文可知道它指的是一种狗，但对于缺乏上下文的文本编码器来说，极有可能把它当成拳击运动员。只用一个单词去做prompt，会经常出现歧义性的问题。</span></p></li><li><p><span>另一个问题是，在预训练的时候，匹配的文本一般都是一个句子，很少出现一个单词的情况，如果推理的时候，每次进来的是一个单词，可能就</span><span style="color:blue; font-family:仿宋; font-weight:bold"><span>存在distribution gap的问题</span></span><span>，抽出来的特征可能就不好。</span></p></li></ul><p><kbd style="border:1px double black; font-size:20px; color: #990000; font-family: comic sans ms, 微软雅黑; font-weight:bold; border-bottom: 2px solid black; border-top: 2px solid black;"><span>做法</span></kbd><span> 基于这两个问题，作者就提出了一种方式去做prompt template，利用这个模板“A photo of a {label}.” 把单词变成一个句子，把单词改成句子了，就不太会出现distribution gap的问题了，而且它的意思是“这是一张xxx的图片”，这个标签一般代表的都是名词，能一定程度上解决歧义性的问题。比如remote这个单词，就指的是遥控器，而不是遥远的。用上了提示模板之后，准确度提升了1.3%。</span></p><p><strong style="color:#6f0670; font-size:18px"><span>提示文本的好处</span></strong><span> 通过为每个任务自定义提示文本可以显著提高zero shot的能力。We found on several fine-grained image classification datasets that it helped to specify the category. 例如做Oxford-IIIT Pets这个数据集，它里面的类别肯定都是动物，给出的提示可以是这样的，“A photo of a {label}, a type of pet.” 这样就缩小了解空间，同样地在Food101数据集上可以指定“a type of food”，在FGVC Aircraft数据集上可以指定“a type of aircraft”，对于OCR数据集，作者发现在要识别的文本或数字旁加上引号可以提高性能。对卫星图像数据集，使用“a satellite photo of a {label}”这样的提示。</span></p><p><strong style="color:#6f0670; font-size:18px"><span>提示文本集成</span></strong><span> 作者还尝试集成多个zero shot classifiers，即prompt ensembling ，作为提高性能的另一种方式。这些分类器是在不同的上下文提示下得到的，比如“A photo of a big {label}&quot; 和”A photo of a small {label}&quot;。We construct the ensemble over the embedding space instead of probability space. This allows us to cache a single set of averaged text embeddings so that the compute cost of the ensemble is the same as using a single classifier when amortized over many predictions. 在ImageNet上，共集成了80个不同的context prompts，这比单个的default prompt 提高了3.5%的性能。</span></p><p><span>作者在 Prompt_Engineering_for_ImageNet.ipynb 列出了使用的这80个context prompts，比如有&quot;a photo of many {}&quot;适合包含多个物体的情况，&quot;a photo of the hard to see {}&quot;可能适合一些小目标或比较难辨认的目标。</span></p><p><img src=".\pp011_files\image-20240924112058080.png" referrerpolicy="no-referrer" alt="image-20240924112058080"></p><h5 id='analysis-of-zero-shot-clip-preformance'><span>Analysis of zero-shot clip preformance</span></h5><p><span>性能分析部分。</span><kbd style="background:yellow; color:red"><span>博文2</span></kbd><span>没有给出！</span></p><h4 id='大范围数据集结果'><span>大范围数据集结果</span></h4><p><span>在imagenet上即使是zero-shot也可以达到ResNet-50的效果，迁移能力令人震惊，但是在一些难以用文本描述的数据集上，如纹理数据集，表现效果较差，且在各数据集上的效果与SOTA比还是有差距，不过其他的都是监督学习，所以总体来看效果还是十分不错。</span></p><p><span>做了27个数据集的分类任务，baseline是ResNet-50，ResNet-50是有监督模型在各个数据集上训练好的， 然后两个模型在其他数据集上zero-shot；</span></p><p><img src=".\pp011_files\image-20240917230249279.png" referrerpolicy="no-referrer" alt="image-20240917230249279"></p><center><span style="color:red">CLIP在各数据集上的zero-shot效果与ResNet50有监督效果对比（采用Linear Probe）</span></center><p><span>在大多数分类任务，给车、食物等做分类的问题上CLIP都表现的很好， 但是在DTD这种纹理进行分类或CLEVRCounts给物体计数的任务，对于CLIP无监督模型来说就很难了；</span></p><p><strong style="color:red"><span>※</span></strong><span> 所以作者认为在这些更难的数据集做few-shot可能比zero-shot更好；</span></p><p><span>推理过程中最关键的一点，在于我们</span><span style="text-decoration: underline 1px solid red; text-decoration-style: double"><span>有很高的自由度去设置“多项选择题”</span></span><span>。从前的分类网络的类别数量是固定的，一般最后一层是跟着 softmax 的全连接层；如果要更改类别数量，就要更换最后一层；并且预测的内容是固定的，不能超过训练集的类别范围。</span></p><p><span>但对于 CLIP 来说，提供给网络的分类标签不仅数量不固定，内容也是自由的。如果提供两个标签，那就是一个二分类问题；如果提供1000个标签，那就是1000分类问题。标签内容可以是常规的分类标签，也可以是一些冷门的分类标签。</span><kbd style="background:yellow; color:red"><span>博文1</span></kbd><span>我认为这是 </span><span style="color:blue; font-family:Comic Sans MS, 微软雅黑"><span>CLIP 的一大主要贡献——</span><strong><span>摆脱了事先定好的分类标签</span></strong></span><span>。</span></p><p>&nbsp;</p><h4 id='模型的泛化性'><span>模型的泛化性</span></h4><p><span>当数据有distribution shift的时候，模型的表现如何，这是CLIP最惊艳的结果：</span></p><p><img src=".\pp011_files\image-20240917230536589.png" referrerpolicy="no-referrer" alt="image-20240917230536589"></p><ul><li><p><span>可以看出CLIP在数据分布的偏移样本上，远远超过ResNet101，而且结果保持地依旧稳健；</span></p></li><li><p><span>以及作者构造了</span><kbd style="border:1px solid #990000; font-size:20px; color: blue; font-family: comic sans ms, 微软雅黑; font-weight:bold"><span>WebImageText (WIT) 数据集</span></kbd><span>。包含4亿个图片-文本对。</span></p></li></ul><p>&nbsp;</p><h3 id='不足'><span>不足</span></h3><ul><li><p><span>Zero-shot的CLIP比基于ResNet-50特征的线性分类器相比具有优势，但在很多任务上，仍逊色于SOTA模型。</span></p></li><li><p><span>CLIP在</span><strong style="color:red;"><span>细分类数据集</span></strong><span>上表示不好；CLIP不擅长处理抽象任务，比如数一数图像中物体的个数；CLIP对一些不包含预训练集中的新型任务，表现也不好， 比如对一张图像中到最近汽车的距离进行分类。</span></p></li><li><p><span>对于一些真正的分布外的数据，CLIP的泛化性能很差。</span></p></li><li><p><span>CLIP本质上还是在有限的类别中进行推理，相比于image caption直接能生成新的输出，还是具有局限性的。一个值得尝试的简单想法是</span><span style="border-bottom: 2px dashed FireBrick;"><span>将对比和生成目标进行联合训练，整合CLIP的有效性和caption模型的灵活性</span></span><span>。</span></p></li><li><p><span>CLIP仍然没有解决深度学习中的</span><code>poor data efficiency</code><span>问题。CLIP与自监督和自训练结合训练会是一个提高数据效率方面的方向。</span></p></li><li><p><span>CLIP 虽然一直强调zero-shot，但是在训练过程中，也反复以数据集的validation performance指导CLIP的表现，并不算真实的zero shot。如果能够创造一个验证zero-shot的迁移能力的新数据集，将会解决这种问题。</span></p></li><li><p><span>4亿图像文本对，不论图像和文本都是从网上爬下来的。而是这些图像文本对没有进行过滤和处理，难免会携带一些社会性偏见。</span></p></li><li><p><span>很多复杂的任务和视觉概念很难仅仅通过文本指定。未来的工作需要进一步开发一种将CLIP强大的zero shot性能与few shot学习相结合的方法。</span></p></li></ul><p>&nbsp;</p><p><span>作者想要：</span></p><pre class="md-fences md-end-block md-fences-with-lineno ty-contain-cm modeLoaded" spellcheck="false" lang=""><div class="CodeMirror cm-s-inner cm-s-null-scroll CodeMirror-wrap" lang=""><div style="overflow: hidden; position: relative; width: 3px; height: 0px; top: 9.26172px; left: 36px;"><textarea autocorrect="off" autocapitalize="off" spellcheck="false" tabindex="0" style="position: absolute; bottom: -1em; padding: 0px; width: 1000px; height: 1em; outline: none;"></textarea></div><div class="CodeMirror-scrollbar-filler" cm-not-content="true"></div><div class="CodeMirror-gutter-filler" cm-not-content="true"></div><div class="CodeMirror-scroll" tabindex="-1"><div class="CodeMirror-sizer" style="margin-left: 28px; margin-bottom: 0px; border-right-width: 0px; padding-right: 0px; padding-bottom: 0px;"><div style="position: relative; top: 0px;"><div class="CodeMirror-lines" role="presentation"><div role="presentation" style="position: relative; outline: none;"><div class="CodeMirror-measure"><pre><span>xxxxxxxxxx</span></pre><div class="CodeMirror-linenumber CodeMirror-gutter-elt"><div>1</div></div></div><div class="CodeMirror-measure"></div><div style="position: relative; z-index: 1;"></div><div class="CodeMirror-code" role="presentation"><div class="CodeMirror-activeline" style="position: relative;"><div class="CodeMirror-activeline-background CodeMirror-linebackground"></div><div class="CodeMirror-gutter-background CodeMirror-activeline-gutter" style="left: -28px; width: 28px;"></div><div class="CodeMirror-gutter-wrapper CodeMirror-activeline-gutter" style="left: -28px;"><div class="CodeMirror-linenumber CodeMirror-gutter-elt CodeMirror-linenumber-show" style="left: 0px; width: 19px;">1</div></div><pre class=" CodeMirror-line " role="presentation"><span role="presentation" style="padding-right: 0.1px;">把一切都GPT（生成式模型）化，因为CLIP还是根据给定的1000个选项去选择到底是那个类别，作者更想直接一张图片，然后生成对应的标题。但受限于计算资源，作者没法做成“自动生成模型”的网络。(以后的DALL)</span></pre></div></div></div></div></div></div><div style="position: absolute; height: 0px; width: 1px; border-bottom: 0px solid transparent; top: 46px;"></div><div class="CodeMirror-gutters" style="height: 46px;"><div class="CodeMirror-gutter CodeMirror-linenumbers" style="width: 27px;"></div></div></div></div></pre><p>&nbsp;</p><h3 id='总结'><span>总结</span></h3><p><span>CLIP 的最大贡献在于打破了固定种类标签的桎梏，让下游任务的推理变得更灵活。</span></p><p><span>在 zero-shot 的情况下效果很不错。可以拓展到其他领域应用，包括物体检测、物体分割、图像生成、视频动作检索等。</span></p><p>&nbsp;</p><h3 id='补充'><span>补充</span></h3><p><span>其它好玩的应用：</span></p><ul><li><p><span>StyleCLIP：根据输入的文本进行人脸编辑</span></p></li><li><p><span>CLIPDraw：根据输入的文本生成画</span></p></li><li><p><span>CLIPS：根据输入的文本从视频中检索到相关物体</span></p></li></ul><p>&nbsp;</p><h3 id='参考博文'><span>参考博文</span></h3><ol start='' ><li><p><a href='https://blog.csdn.net/weixin_57974242/article/details/134196877'><span>【论文阅读】Learning Transferable Visual Models From Natural Language Supervision</span></a><span> </span></p><p><span style="color:red"><span>点评：★★★★☆，博文有自己的总结和思考，帮助我第一遍理清了一些CLIP的相关知识。还不错</span></span><span>！</span></p></li><li><p><a href='https://zhuanlan.zhihu.com/p/511460120'><span>CLIP论文 | Learning Transferable Visual Models From Natural Language Supervision</span></a><span> </span></p><p><span style="color:red"><span>点评：这篇博文简直很棒！用例说的非常详细，就应该写出这样的博文才是赞的！</span></span><span> </span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red; background: yellow; color:red"><span>★★★★★</span></span></p></li><li><p><a href='https://0809zheng.github.io/2021/01/06/dalle.html'><span>Learning Transferable Visual Models From Natural Language Supervision</span></a><span> </span></p><p><span style="color:red"><span>点评：补充了一些背景知识，</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red; background: yellow"><span>★★★☆☆</span></span></span><span>。</span></p></li><li><p><a href='https://www.cnblogs.com/lhiker/p/15903877.html'><span>CLIP：Learning Transferable Visual Models From Natural Language Supervision</span></a><span> </span></p><p><span style="color:red"><span>点评：作者对一些概念和文字进一步叙述了一下，能够帮助读者进一步理解，还不错，★★★☆☆</span></span><span>。</span></p></li><li><p><a href='https://blog.csdn.net/weixin_44031582/article/details/120469669'><span>CLIP论文笔记--《Learning Transferable Visual Models From Natural Language Supervision》</span></a><span> </span></p><p><strong style="color:red"><span>※</span></strong><span> 这篇文章主要说的内容还是不错的！但是我</span><span style="color:red"><span>没有详细看</span></span><span>，等后面需要用到或者想要进一步研究的时候，可以深入看看这篇博文。暂定</span><span style="border-radius: 10px; border-top: 1px solid red; border-bottom: 1px solid red; border-left: 1px solid red; border-right: 1px solid red; background: yellow"><span>★★★★★</span></span><span>。2024年9月24日 11:39:10</span></p></li></ol><p>&nbsp;</p><div class='md-toc' mdtype='toc'><p class="md-toc-content" role="list"><span role="listitem" class="md-toc-item md-toc-h1" data-ref="n0"><a class="md-toc-inner" href="#clip2021---论文精读学习笔记">CLIP - 论文精读学习笔记</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n12"><a class="md-toc-inner" href="#全文概述">全文概述</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n52"><a class="md-toc-inner" href="#摘要">摘要</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n55"><a class="md-toc-inner" href="#背景">背景</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n71"><a class="md-toc-inner" href="#方法">方法</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n79"><a class="md-toc-inner" href="#clip是如何进行预训练的">CLIP是如何进行预训练的？</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n81"><a class="md-toc-inner" href="#clip是如何做zero-shot的推理的">CLIP是如何做zero-shot的推理的？</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n84"><a class="md-toc-inner" href="#natural-language-supervision">Natural Language Supervision</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n89"><a class="md-toc-inner" href="#creating-a-sufficiently-large-dataset---数据集">Creating a Sufficiently Large Dataset - 数据集</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n98"><a class="md-toc-inner" href="#为什么clip要采用对比学习的方法">为什么CLIP要采用对比学习的方法</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n107"><a class="md-toc-inner" href="#训练">训练</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n137"><a class="md-toc-inner" href="#整体架构">整体架构</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n138"><a class="md-toc-inner" href="#choosing-and-scaling-a-model">Choosing and Scaling a Model</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n140"><a class="md-toc-inner" href="#训练-2">训练</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n148"><a class="md-toc-inner" href="#主干模型">主干模型</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n158"><a class="md-toc-inner" href="#推理过程">推理过程</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n176"><a class="md-toc-inner" href="#代码">代码</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n187"><a class="md-toc-inner" href="#与之前zero-shot模型的对比">与之前Zero-shot模型的对比</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n195"><a class="md-toc-inner" href="#实验">实验</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n196"><a class="md-toc-inner" href="#zero-shot-transfer">Zero-Shot Transfer</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n197"><a class="md-toc-inner" href="#动机">动机</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n200"><a class="md-toc-inner" href="#using-clip-for-zero-shot-transfer">Using CLIP for zero-shot transfer</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n203"><a class="md-toc-inner" href="#initial-comparison-to-visual-n-grams">Initial comparison to visual N-Grams</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n206"><a class="md-toc-inner" href="#prompt-engineering-and-ensembling">Prompt engineering and ensembling</a></span><span role="listitem" class="md-toc-item md-toc-h5" data-ref="n219"><a class="md-toc-inner" href="#analysis-of-zero-shot-clip-preformance">Analysis of zero-shot clip preformance</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n221"><a class="md-toc-inner" href="#大范围数据集结果">大范围数据集结果</a></span><span role="listitem" class="md-toc-item md-toc-h4" data-ref="n231"><a class="md-toc-inner" href="#模型的泛化性">模型的泛化性</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n240"><a class="md-toc-inner" href="#不足">不足</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n262"><a class="md-toc-inner" href="#总结">总结</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n266"><a class="md-toc-inner" href="#补充">补充</a></span><span role="listitem" class="md-toc-item md-toc-h3" data-ref="n276"><a class="md-toc-inner" href="#参考博文">参考博文</a></span></p></div><p><kbd style="border:1px double black; font-size:20px; color: #990000; font-family: comic sans ms, 微软雅黑; font-weight:bold; border-bottom: 2px solid black; border-top: 2px solid black;"><span>博文免责声明</span></kbd><span> </span></p><ol start='' ><li><p><span>本条博文信息主要整合自网络，部分内容为自己的理解写出来的，如有断章截句导致不正确或因个人水平有限未能详尽正确描述的地方，敬请各位读者指正；</span></p></li><li><p><span>引用出处可能没有完全追溯到原始来源，如因此冒犯到原创作者，请</span><a href='https://mustbook.github.io/'><span>联系本人</span></a><span>更正/删除；</span></p></li><li><p><span>博文的发布主要用于自我学习，其次希望帮助到有共同疑惑的朋友。</span></p></li></ol><div style="
    border-radius: 25px; 
    border: 2px solid #990000;
    background: #990000;
    padding: 20px;
"><center><span style="color:white">欢迎随时联系讨论，一起成长进步。</span></center></div><p>&nbsp;</p><div class='footnotes-area'  ><hr/>
<div class='footnote-line'><span class='md-fn-count'>1</span> <span>Radford A, Kim J W, Hallacy C, et al. Learning transferable visual models from natural language supervision[C]. International conference on machine learning, 2021: 8748-8763.</span> <a name='dfref-footnote-1' href='#ref-footnote-1' title='back to document' class='reversefootnote' >↩</a></div></div></div></div>
<a href=".typora-export-content" id="scroll-up" style="display: block;">
		<i class="material-icons md-20 md-middle"></i>
	</a></body>
</html>